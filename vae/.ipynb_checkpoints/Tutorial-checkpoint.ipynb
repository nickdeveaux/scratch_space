{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import imsave\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "distributions = tf.contrib.distributions\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('data_dir', '/tmp/dat/', 'Directory for data')\n",
    "flags.DEFINE_string('logdir', '/tmp/log/', 'Directory for logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For making plots:\n",
    "flags.DEFINE_integer('latent_dim', 2, 'Latent dimensionality of model')\n",
    "flags.DEFINE_integer('batch_size', 64, 'Minibatch size')\n",
    "flags.DEFINE_integer('n_samples', 10, 'Number of samples to save')\n",
    "flags.DEFINE_integer('print_every', 10, 'Print every n iterations')\n",
    "flags.DEFINE_integer('hidden_size', 200, 'Hidden size for neural networks')\n",
    "flags.DEFINE_integer('n_iterations', 1000, 'number of iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def inference_network(x, latent_dim, hidden_size):\n",
    "  \"\"\"Construct an inference network parametrizing a Gaussian.\n",
    "  Args:\n",
    "    x: A batch of MNIST digits.\n",
    "    latent_dim: The latent dimensionality.\n",
    "    hidden_size: The size of the neural net hidden layers.\n",
    "  Returns:\n",
    "    mu: Mean parameters for the variational family Normal\n",
    "    sigma: Standard deviation parameters for the variational family Normal\n",
    "  \"\"\"\n",
    "  with slim.arg_scope([slim.fully_connected], activation_fn=tf.nn.relu):\n",
    "    net = slim.flatten(x)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    gaussian_params = slim.fully_connected(\n",
    "        net, latent_dim * 2, activation_fn=None)\n",
    "  # The mean parameter is unconstrained\n",
    "  mu = gaussian_params[:, :latent_dim]\n",
    "  # The standard deviation must be positive. Parametrize with a softplus\n",
    "  sigma = tf.nn.softplus(gaussian_params[:, latent_dim:])\n",
    "  return mu, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/dat/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/dat/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/dat/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/dat/t10k-labels-idx1-ubyte.gz\n",
      "Saving TensorBoard summaries and images to: /tmp/log/\n",
      "Iteration: 0 ELBO: -540.093 Examples/s: 2.482e+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py27.13/lib/python2.7/site-packages/ipykernel_launcher.py:119: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/anaconda/envs/py27.13/lib/python2.7/site-packages/ipykernel_launcher.py:122: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/anaconda/envs/py27.13/lib/python2.7/site-packages/ipykernel_launcher.py:125: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/anaconda/envs/py27.13/lib/python2.7/site-packages/ipykernel_launcher.py:155: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ELBO: -244.866 Examples/s: 2.133e+05\n",
      "Iteration: 20 ELBO: -215.574 Examples/s: 1.897e+05\n",
      "Iteration: 30 ELBO: -208.272 Examples/s: 2.594e+05\n",
      "Iteration: 40 ELBO: -212.581 Examples/s: 2.817e+05\n",
      "Iteration: 50 ELBO: -224.426 Examples/s: 2.518e+05\n",
      "Iteration: 60 ELBO: -203.580 Examples/s: 2.403e+05\n",
      "Iteration: 70 ELBO: -189.765 Examples/s: 2.563e+05\n",
      "Iteration: 80 ELBO: -209.038 Examples/s: 2.524e+05\n",
      "Iteration: 90 ELBO: -205.015 Examples/s: 2.392e+05\n",
      "Iteration: 100 ELBO: -195.272 Examples/s: 2.566e+05\n",
      "Iteration: 110 ELBO: -196.799 Examples/s: 2.551e+05\n",
      "Iteration: 120 ELBO: -201.701 Examples/s: 2.702e+05\n",
      "Iteration: 130 ELBO: -180.258 Examples/s: 1.881e+05\n",
      "Iteration: 140 ELBO: -183.292 Examples/s: 2.018e+05\n",
      "Iteration: 150 ELBO: -206.210 Examples/s: 2.471e+05\n",
      "Iteration: 160 ELBO: -190.812 Examples/s: 2.460e+05\n",
      "Iteration: 170 ELBO: -183.936 Examples/s: 2.552e+05\n",
      "Iteration: 180 ELBO: -187.468 Examples/s: 1.973e+05\n",
      "Iteration: 190 ELBO: -179.380 Examples/s: 2.031e+05\n",
      "Iteration: 200 ELBO: -186.011 Examples/s: 2.377e+05\n",
      "Iteration: 210 ELBO: -187.391 Examples/s: 2.374e+05\n",
      "Iteration: 220 ELBO: -179.541 Examples/s: 2.459e+05\n",
      "Iteration: 230 ELBO: -175.287 Examples/s: 2.548e+05\n",
      "Iteration: 240 ELBO: -179.545 Examples/s: 2.102e+05\n",
      "Iteration: 250 ELBO: -167.246 Examples/s: 2.560e+05\n",
      "Iteration: 260 ELBO: -179.419 Examples/s: 2.295e+05\n",
      "Iteration: 270 ELBO: -173.304 Examples/s: 2.502e+05\n",
      "Iteration: 280 ELBO: -169.422 Examples/s: 2.814e+05\n",
      "Iteration: 290 ELBO: -167.764 Examples/s: 2.547e+05\n",
      "Iteration: 300 ELBO: -175.183 Examples/s: 2.543e+05\n",
      "Iteration: 310 ELBO: -173.156 Examples/s: 2.510e+05\n",
      "Iteration: 320 ELBO: -182.771 Examples/s: 2.811e+05\n",
      "Iteration: 330 ELBO: -174.122 Examples/s: 2.101e+05\n",
      "Iteration: 340 ELBO: -166.664 Examples/s: 2.429e+05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b4aba092c3ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/py27.13/lib/python2.7/site-packages/tensorflow/python/platform/app.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b4aba092c3ac>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeleteRecursively\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b4aba092c3ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mnp_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mnp_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp_x\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp_x\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# Print progress and save samples every so often\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py27.13/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py27.13/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py27.13/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/envs/py27.13/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py27.13/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def generative_network(z, hidden_size):\n",
    "  \"\"\"Build a generative network parametrizing the likelihood of the data\n",
    "  Args:\n",
    "    z: Samples of latent variables\n",
    "    hidden_size: Size of the hidden state of the neural net\n",
    "  Returns:\n",
    "    bernoulli_logits: logits for the Bernoulli likelihood of the data\n",
    "  \"\"\"\n",
    "  with slim.arg_scope([slim.fully_connected], activation_fn=tf.nn.relu):\n",
    "    net = slim.fully_connected(z, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    bernoulli_logits = slim.fully_connected(net, 784, activation_fn=None)\n",
    "    bernoulli_logits = tf.reshape(bernoulli_logits, [-1, 28, 28, 1])\n",
    "  return bernoulli_logits\n",
    "\n",
    "\n",
    "def train():\n",
    "  # Train a Variational Autoencoder on MNIST\n",
    "\n",
    "  # Input placeholders\n",
    "  with tf.name_scope('data'):\n",
    "    x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    tf.summary.image('data', x)\n",
    "\n",
    "  with tf.variable_scope('variational'):\n",
    "    q_mu, q_sigma = inference_network(x=x,\n",
    "                                      latent_dim=FLAGS.latent_dim,\n",
    "                                      hidden_size=FLAGS.hidden_size)\n",
    "    # The variational distribution is a Normal with mean and standard\n",
    "    # deviation given by the inference network\n",
    "    q_z = distributions.Normal(loc=q_mu, scale=q_sigma)\n",
    "    assert q_z.reparameterization_type == distributions.FULLY_REPARAMETERIZED\n",
    "\n",
    "  with tf.variable_scope('model'):\n",
    "    # The likelihood is Bernoulli-distributed with logits given by the\n",
    "    # generative network\n",
    "    p_x_given_z_logits = generative_network(z=q_z.sample(),\n",
    "                                            hidden_size=FLAGS.hidden_size)\n",
    "    p_x_given_z = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "    posterior_predictive_samples = p_x_given_z.sample()\n",
    "    tf.summary.image('posterior_predictive',\n",
    "                     tf.cast(posterior_predictive_samples, tf.float32))\n",
    "\n",
    "  # Take samples from the prior\n",
    "  with tf.variable_scope('model', reuse=True):\n",
    "    p_z = distributions.Normal(loc=np.zeros(FLAGS.latent_dim, dtype=np.float32),\n",
    "                               scale=np.ones(FLAGS.latent_dim, dtype=np.float32))\n",
    "    p_z_sample = p_z.sample(FLAGS.n_samples)\n",
    "    p_x_given_z_logits = generative_network(z=p_z_sample,\n",
    "                                            hidden_size=FLAGS.hidden_size)\n",
    "    prior_predictive = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "    prior_predictive_samples = prior_predictive.sample()\n",
    "    tf.summary.image('prior_predictive',\n",
    "                     tf.cast(prior_predictive_samples, tf.float32))\n",
    "\n",
    "  # Take samples from the prior with a placeholder\n",
    "  with tf.variable_scope('model', reuse=True):\n",
    "    z_input = tf.placeholder(tf.float32, [None, FLAGS.latent_dim])\n",
    "    p_x_given_z_logits = generative_network(z=z_input,\n",
    "                                            hidden_size=FLAGS.hidden_size)\n",
    "    prior_predictive_inp = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "    prior_predictive_inp_sample = prior_predictive_inp.sample()\n",
    "\n",
    "  # Build the evidence lower bound (ELBO) or the negative loss\n",
    "  kl = tf.reduce_sum(distributions.kl(q_z, p_z), 1)\n",
    "  expected_log_likelihood = tf.reduce_sum(p_x_given_z.log_prob(x),\n",
    "                                          [1, 2, 3])\n",
    "\n",
    "  elbo = tf.reduce_sum(expected_log_likelihood - kl, 0)\n",
    "\n",
    "  optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "\n",
    "  train_op = optimizer.minimize(-elbo)\n",
    "\n",
    "  # Merge all the summaries\n",
    "  summary_op = tf.summary.merge_all()\n",
    "\n",
    "  init_op = tf.global_variables_initializer()\n",
    "\n",
    "  # Run training\n",
    "  sess = tf.InteractiveSession()\n",
    "  sess.run(init_op)\n",
    "\n",
    "  mnist = read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "  print('Saving TensorBoard summaries and images to: %s' % FLAGS.logdir)\n",
    "  train_writer = tf.summary.FileWriter(FLAGS.logdir, sess.graph)\n",
    "\n",
    "  # Get fixed MNIST digits for plotting posterior means during training\n",
    "  np_x_fixed, np_y = mnist.test.next_batch(5000)\n",
    "  np_x_fixed = np_x_fixed.reshape(5000, 28, 28, 1)\n",
    "  np_x_fixed = (np_x_fixed > 0.5).astype(np.float32)\n",
    "\n",
    "  for i in range(FLAGS.n_iterations):\n",
    "    # Re-binarize the data at every batch; this improves results\n",
    "    np_x, _ = mnist.train.next_batch(FLAGS.batch_size)\n",
    "    np_x = np_x.reshape(FLAGS.batch_size, 28, 28, 1)\n",
    "    np_x = (np_x > 0.5).astype(np.float32)\n",
    "    sess.run(train_op, {x: np_x})\n",
    "\n",
    "    # Print progress and save samples every so often\n",
    "    t0 = time.time()\n",
    "    if i % FLAGS.print_every == 0:\n",
    "      np_elbo, summary_str = sess.run([elbo, summary_op], {x: np_x})\n",
    "      train_writer.add_summary(summary_str, i)\n",
    "      print('Iteration: {0:d} ELBO: {1:.3f} Examples/s: {2:.3e}'.format(\n",
    "          i,\n",
    "          np_elbo / FLAGS.batch_size,\n",
    "          FLAGS.batch_size * FLAGS.print_every / (time.time() - t0)))\n",
    "      t0 = time.time()\n",
    "\n",
    "      # Save samples\n",
    "      np_posterior_samples, np_prior_samples = sess.run(\n",
    "          [posterior_predictive_samples, prior_predictive_samples], {x: np_x})\n",
    "      for k in range(FLAGS.n_samples):\n",
    "        f_name = os.path.join(\n",
    "            FLAGS.logdir, 'iter_%d_posterior_predictive_%d_data.jpg' % (i, k))\n",
    "        imsave(f_name, np_x[k, :, :, 0])\n",
    "        f_name = os.path.join(\n",
    "            FLAGS.logdir, 'iter_%d_posterior_predictive_%d_sample.jpg' % (i, k))\n",
    "        imsave(f_name, np_posterior_samples[k, :, :, 0])\n",
    "        f_name = os.path.join(\n",
    "            FLAGS.logdir, 'iter_%d_prior_predictive_%d.jpg' % (i, k))\n",
    "        imsave(f_name, np_prior_samples[k, :, :, 0])\n",
    "\n",
    "      # Plot the posterior predictive space\n",
    "      if FLAGS.latent_dim == 2:\n",
    "        np_q_mu = sess.run(q_mu, {x: np_x_fixed})\n",
    "        cmap = mpl.colors.ListedColormap(sns.color_palette(\"husl\"))\n",
    "        f, ax = plt.subplots(1, figsize=(6 * 1.1618, 6))\n",
    "        im = ax.scatter(np_q_mu[:, 0], np_q_mu[:, 1], c=np.argmax(np_y, 1), cmap=cmap,\n",
    "                        alpha=0.7)\n",
    "        ax.set_xlabel('First dimension of sampled latent variable $z_1$')\n",
    "        ax.set_ylabel('Second dimension of sampled latent variable mean $z_2$')\n",
    "        ax.set_xlim([-10., 10.])\n",
    "        ax.set_ylim([-10., 10.])\n",
    "        f.colorbar(im, ax=ax, label='Digit class')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FLAGS.logdir,\n",
    "                                 'posterior_predictive_map_frame_%d.jpg' % i))\n",
    "        plt.close()\n",
    "\n",
    "        nx = ny = 20\n",
    "        x_values = np.linspace(-3, 3, nx)\n",
    "        y_values = np.linspace(-3, 3, ny)\n",
    "        canvas = np.empty((28 * ny, 28 * nx))\n",
    "        for ii, yi in enumerate(x_values):\n",
    "          for j, xi in enumerate(y_values):\n",
    "            np_z = np.array([[xi, yi]])\n",
    "            x_mean = sess.run(prior_predictive_inp_sample, {z_input: np_z})\n",
    "            canvas[(nx - ii - 1) * 28:(nx - ii) * 28, j *\n",
    "                   28:(j + 1) * 28] = x_mean[0].reshape(28, 28)\n",
    "        imsave(os.path.join(FLAGS.logdir,\n",
    "                            'prior_predictive_map_frame_%d.jpg' % i), canvas)\n",
    "        # plt.figure(figsize=(8, 10))\n",
    "        # Xi, Yi = np.meshgrid(x_values, y_values)\n",
    "        # plt.imshow(canvas, origin=\"upper\")\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig()\n",
    "\n",
    "  # Make the gifs\n",
    "  if FLAGS.latent_dim == 2:\n",
    "    os.system(\n",
    "        'convert -delay 15 -loop 0 {0}/posterior_predictive_map_frame*png {0}/posterior_predictive.gif'\n",
    "        .format(FLAGS.logdir))\n",
    "    os.system(\n",
    "        'convert -delay 15 -loop 0 {0}/prior_predictive_map_frame*png {0}/prior_predictive.gif'\n",
    "        .format(FLAGS.logdir))\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if tf.gfile.Exists(FLAGS.logdir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.logdir)\n",
    "  tf.gfile.MakeDirs(FLAGS.logdir)\n",
    "  train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.distributions.kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
